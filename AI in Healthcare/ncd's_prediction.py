# -*- coding: utf-8 -*-
"""NCD's prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16imUaHZC4Ga3QKI_mfLKMD00yplPsVsV
"""



"""# Machine Learning for predicting risk scores of having NCDs(Heart & Diabetes)

---


**<h3>Activities</h3>**
<ol type="1">
    <li>Data collection, loading </li>
    <li>Pre-Processing
    <ul>
           <li>Data cleaning</li>
           <li>Understanding the variables </li>
            <li> Checking nan values  </li>
            <li> detecting outliers </li>
            <li> checking duplicates </li>
         </ul>
    </li>
     <li>Exploratory Data Analysis </li>
     <li>Label Encoding </li>
      <li>Feature Engineering  </li>
    <li>Build ML models using percentage split
         <ul>
            <li> Random Forest  </li>
            <li> Support Vector Machine </li>
            <li> Artificial Neural Networks  </li>   
            <li>  Naive Bayes </li>
         </ul>
    </li>
    <li>Model Predictions  </li>
    <li>Model Evaluation </li>
    <li>Communicating the insights </li>
</ol>

# Importing libraries
"""

# !pip install colabcode
# !pip install fastapi

# from colabcode import ColabCode
# from fastapi import FastAPI


# app = FastAPI(title="ML Models as API on Google Colab", description="with FastAPI and ColabCode", version="1.0")

# cc = ColabCode(port=12000, code=False)

# app = FastAPI()

# @app.get("/")
# async def read_root():
#   return {"Hello": "world"}

# for data loading, assembly, manipulation, and exploration.
import pandas as pd
import numpy as np

# for visualization
import seaborn as sns
import matplotlib.pyplot as plt

# for statistical models
import statsmodels.api as sm

# for model training and testing
from sklearn.model_selection import train_test_split

# for model selection and fitting
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC

# for performance metrics
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score

# for creating tables
from tabulate import tabulate

from sklearn.preprocessing import LabelEncoder, StandardScaler

# for distance calculation
from sklearn.neighbors import DistanceMetric

# for curve fiting the model
from scipy.optimize import curve_fit

# ignoring warnings
import warnings
warnings.filterwarnings("ignore")

"""# Heart Disease

# 1. Data collection and loading
"""

heart_data = pd.read_csv("Heart_Disease.csv")
heart_data

heart_data.sample(20)

"""# 2. Pre-Processing"""

heart_data.info()

"""## Checking null values"""

heart_data.isnull().sum()

"""The heart dataset has no missing values"""

# check for duplicates
heart_data.duplicated().sum()

"""The dataset has no duplicates"""

# analyzing the target variable
heart_data['Heart Disease'].value_counts()

# understanding the columns
heart_data.columns

# understanding the data numerically (summary statistics)
heart_data.describe()

"""From the above statistics, we can note the following:
- Cholesterol holds a high frequency in the data, with the max 564 and the minimum of 126 and a highest mean

"""



"""# 3. Exploratory Data Analysis"""

# correlation matrix of the weather variables
corr_m = heart_data.corr()
# corr_m
# ploting the heatmap for the correlation
plt.figure(figsize=(18,10))          # Adjusting the plot size
sns.heatmap(corr_m,cmap='crest', annot=True)

plt.title("Correlation matrix of heart variables")
plt.show()

# Univariate analysis: Create box plots to identify the presence of any outliers

fig, axs = plt.subplots(1, 4, figsize=(15, 5))

for i, col in enumerate(['BP','Cholesterol','Chest pain type','Max HR']):
    axs[i].boxplot(heart_data[col])
    axs[i].set_title(col)
    axs[i].set_ylabel('Value')


plt.show()

# getting the frequency of each class
target_column = heart_data['Heart Disease'].value_counts()
# print(appliance_column)

# ploting the frequency of appliances
plt.hist(heart_data['Heart Disease'])
plt.xlabel('Heart disease')
plt.ylabel('Frequency')
plt.title('Distribution of Heart Disease')
plt.show()

heart_data['Heart Disease'].value_counts()

# plotting the distribution of the sex
sex_counts = heart_data['Sex'].value_counts()
male_count = sex_counts[1]
female_count = sex_counts[0]

labels = ['Male', 'Female']
values = [male_count, female_count]

plt.bar(labels, values,color=['#4e79a7', '#e15759'])
plt.title('Sex Distribution')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()

# group the data by sex and status, and count the number of occurrences
grouped = heart_data.groupby(['Sex', 'Heart Disease'])['Heart Disease'].count().unstack()
print(grouped)

# plot a stacked bar chart
ax = grouped.plot(kind='bar', stacked=True)
ax.set_xticklabels(['Female', 'Male'], rotation=0)
ax.set_xlabel('Sex')
ax.set_ylabel('Count')
ax.set_title('Heart Disease Status Distribution by Gender')
plt.show()

# Create distribution plots for all variables using histogram
for column in heart_data:
    sns.histplot(data=heart_data, x=column, kde=True)
    plt.title("Heart Disease Distribution by "+column)
    plt.show()

"""# 4. Label Encoding"""

# Encode categorical label (Heart Disease)
status_encoder = LabelEncoder()
heart_data['Heart Disease'] = status_encoder.fit_transform(heart_data['Heart Disease'])

heart_data

"""# 5. Feature engineering -- Feature selction

Feature selection is done to avoid overfitting by removing unnecessary features for prediction.

it is done by using stepwise regression -- Forward selection

The forward selection will start with an empty set of variables, then add one variable at time at a time until a certain criterion is met basing on the most significant using the statistical significance.  In this case it will select features based on the feature that has the best pvalue less than 0.05 threshold. The selected features will further be used for fitting a linear regression model.
"""



#forward regression method from https://github.com/AakkashVijayakumar/stepwise-regression/blob/master/stepwise_regression/step_reg.py
# define the dependent and independent axis
x =  heart_data.iloc[:,:-1]
y = heart_data['Heart Disease']

selected_features=[]

# forward selection
def forward_regression(X, y, verbose=False):

    top_variable = []               # keeps the most significant variables

    while True:
        changed=False
        least_significant = list(set(X.columns)-set(top_variable))      # the least significant variables
        new_pvalue = pd.Series(index=least_significant)
        for values in least_significant:
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[top_variable+[values]]))).fit()
            new_pvalue[values] = model.pvalues[values]
        best_pvalue = new_pvalue.min()
        if best_pvalue < 0.05:                 # the best pvalue is the pvalue with less than 0.05 threshold
            best_variable = new_pvalue.idxmin()
            top_variable.append(best_variable)
            changed=True
            if True:
                print('Add {:20} with p-value {:.6}'.format(best_variable, best_pvalue))
                selected_features.append(best_variable) #storing the added feature

        if not changed:
            break
    print(model.summary())    # print the model summary

forward_regression(x,y,verbose=True)

# selected features
print("Out of 18 features, forward feature selection selected:",len(selected_features))
print(selected_features)

"""# 6. Building models

Random forest
"""

# adjusting the x_train and x_test to be the selected features
x_ = heart_data[selected_features]
y_ = heart_data['Heart Disease']

#Split the data for training and testing
x_train,x_test,y_train,y_test=train_test_split(x_, y_,test_size=0.2, random_state=42)
# random state ensures that the splits that you generate are reproducible

# defining the search area
search_area={'n_estimators':[10,40,70,90,120],'max_depth':[2,3,5,10,20]}

# GridSearch
grid_search=GridSearchCV(estimator=RandomForestClassifier(), param_grid=search_area,cv=10, n_jobs=1,verbose=2,scoring='accuracy')
grid_search=grid_search.fit(x_train,y_train)

# the best estimator obtained by gridsearch
best_estimator= grid_search.best_estimator_
best_estimator

# Fit the best retrieved parameters into a random forest model
randomf = RandomForestClassifier(n_estimators=90 , max_depth=3,random_state=42)
randomf.fit(x_train,y_train)   # fiting the model using training set

# predictions
y_pred = randomf.predict(x_test)
y_pred


# Calculating the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Calculating the precision score  of the model
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Calculating the recall score  of the model
recall = recall_score(y_test, y_pred)
print("Recall:", recall)

#  Calculating the f1_score of the model
f1 = f1_score(y_test, y_pred)
print("F1 Score:", f1)



"""Support Vector Machine"""

import pickle

# Creating an SVM model
svm_model = SVC(kernel='linear', C=1)

# Training the model on the training set
svm_model.fit(x_train, y_train)

with open('svm_model.pkl', 'wb') as f:
    pickle.dump(svm_model, f)

# Making predictions on the testing set
y_predicted = svm_model.predict(x_test)
y_predicted

# Calculating the accuracy of the model
accuracy = accuracy_score(y_test, y_predicted)
print("Accuracy:", accuracy)

# Calculating the precision score  of the model
precision = precision_score(y_test, y_predicted)
print("Precision:", precision)

# Calculating the recall score  of the model
recall = recall_score(y_test, y_predicted)
print("Recall:", recall)

#  Calculating the f1_score of the model
f1 = f1_score(y_test, y_predicted)
print("F1 Score:", f1)

# cc.run_app(app=app)

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
# Creating an NB model
nb_model = GaussianNB()

# Training the model on the training set
nb_model.fit(x_train, y_train)

# Making predictions on the testing set
ny_predicted = nb_model.predict(x_test)
ny_predicted

# Calculating the accuracy of the model
accuracy3 = accuracy_score(y_test, ny_predicted)
print("Accuracy of nb model :", accuracy3)

# Calculating the precision score  of the model
precision = precision_score(y_test, ny_predicted)
print("Precision:", precision)

# Calculating the recall score  of the model
recall = recall_score(y_test, ny_predicted)
print("Recall:", recall)

#  Calculating the f1_score of the model
f1 = f1_score(y_test, ny_predicted)
print("F1 Score:", f1)

"""Artificial Neural Network"""

# Importing necessary libraries
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

import pickle

# Scaling the data
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Creating an artificial neural network model
#Initialising ANN
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=x_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compiling the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Training the model on the training set
history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=16)

with open('ann_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# # Evaluating the model on the testing set
_, accuracy = model.evaluate(x_test, y_test)
print('Accuracy: %.2f%%' % (accuracy*100))

# Making predictions on the testing set
ypredicted = (model.predict(x_test) > 0.5).astype("int32")

# Calculating performance metrics
accuracy = accuracy_score(y_test, ypredicted)
precision = precision_score(y_test, ypredicted)
recall = recall_score(y_test, ypredicted)
f1 = f1_score(y_test, ypredicted)

# Printing the performance metrics
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)



"""## DIABETES PREDICTION"""

# from google.colab import files
# uploaded = files.upload()

diabetes_data = pd.read_csv("diabetes_data_upload.csv")
diabetes_data

diabetes_data.info()

diabetes_data['class'].value_counts()

diabetes_data.duplicated().sum()

diabetes_data = diabetes_data.drop_duplicates(keep='first')
# Reseting the index
diabetes_data = diabetes_data.reset_index(drop=True)

diabetes_data.info()

diabetes_data.duplicated().sum()

diabetes_data.isnull().sum()

diabetes_data['class'].value_counts()

"""Let us encode data to numerical values"""

diabetes_data['Gender'] = diabetes_data['Gender'].map({'Male':1,'Female':0})
diabetes_data['class'] = diabetes_data['class'].map({'Positive':1,'Negative':0})
diabetes_data['Polyuria'] = diabetes_data['Polyuria'].map({'Yes':1,'No':0})
diabetes_data['Polydipsia'] = diabetes_data['Polydipsia'].map({'Yes':1,'No':0})
diabetes_data['sudden weight loss'] = diabetes_data['sudden weight loss'].map({'Yes':1,'No':0})
diabetes_data['weakness'] = diabetes_data['weakness'].map({'Yes':1,'No':0})
diabetes_data['Polyphagia'] = diabetes_data['Polyphagia'].map({'Yes':1,'No':0})
diabetes_data['Genital thrush'] = diabetes_data['Genital thrush'].map({'Yes':1,'No':0})
diabetes_data['visual blurring'] = diabetes_data['visual blurring'].map({'Yes':1,'No':0})
diabetes_data['Itching'] = diabetes_data['Itching'].map({'Yes':1,'No':0})
diabetes_data['Irritability'] = diabetes_data['Irritability'].map({'Yes':1,'No':0})
diabetes_data['delayed healing'] = diabetes_data['delayed healing'].map({'Yes':1,'No':0})
diabetes_data['partial paresis'] = diabetes_data['partial paresis'].map({'Yes':1,'No':0})
diabetes_data['muscle stiffness'] = diabetes_data['muscle stiffness'].map({'Yes':1,'No':0})
diabetes_data['Alopecia'] = diabetes_data['Alopecia'].map({'Yes':1,'No':0})
diabetes_data['Obesity'] = diabetes_data['Obesity'].map({'Yes':1,'No':0})

diabetes_data.info()

diabetes_data.nunique()

diabetes_data.describe()

# correlation matrix of the variables
corr_m = diabetes_data.corr()

# ploting the heatmap for the correlation
plt.figure(figsize=(18,10))          # Adjusting the plot size
sns.heatmap(corr_m,cmap='crest', annot=True)

plt.title("Correlation matrix of the diabetes indicators")
plt.show()

# group the data by sex and status, and count the number of occurrences
grouped = diabetes_data.groupby(['Gender', 'class'])['class'].count().unstack()
print(grouped)

# map the labels to "Presence" and "Absence"
grouped.columns = ['Absence', 'Presence']
# plot a stacked bar chart
ax = grouped.plot(kind='bar', stacked=True)
ax.set_xticklabels(['Female', 'Male'], rotation=0)
ax.set_xlabel('Gender')
ax.set_ylabel('Count')
ax.set_title('Diabetes Disease status Distribution by Gender')

# add the legend labels
handles, labels = ax.get_legend_handles_labels()
labels = ['Absence', 'Presence']
ax.legend(handles, labels)
plt.show()

# Create distribution plots for all variables using histogram
for column in diabetes_data:
    sns.histplot(data=diabetes_data, x=column, kde=True)
    plt.title("Diabetes Disease Distribution by "+column)
    plt.show()

"""Let's now do feature selection"""

x = diabetes_data.iloc[:,:-1]
y = diabetes_data.iloc[:,-1]

# define the dependent and independent axis
selected_features=[]
# forward selection
def forward_regression(X, y, verbose=False):

    top_variable = []

    while True:
        changed=False
        least_significant = list(set(X.columns)-set(top_variable))
        new_pvalue = pd.Series(index=least_significant)
        for values in least_significant:
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[top_variable+[values]]))).fit()
            new_pvalue[values] = model.pvalues[values]
        best_pvalue = new_pvalue.min()
        if best_pvalue < 0.05:
            best_variable = new_pvalue.idxmin()
            top_variable.append(best_variable)
            changed=True
            if True:
                print('Add {:20} with p-value {:.6}'.format(best_variable, best_pvalue))
                selected_features.append(best_variable)

        if not changed:
            break
    # print(model.summary())

forward_regression(x,y,verbose=True)

# Backward stepwise feature selection
def backward_regression(X, y,
                           threshold_out,
                           verbose=True):
    included=list(X.columns)
    while True:
        changed=False
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        # use all coefs except intercept
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max() # null if pvalues is empty
        if worst_pval > threshold_out:
            changed=True
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            if verbose:
                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))
        if not changed:
            break
    # print("P-values for each variable:")
    # print(model.pvalues[1:])
    return included

backward_regression(x,y,0.05)

new_set = diabetes_data[['Gender', 'Polyuria', 'Polydipsia', 'Genital thrush', 'Itching', 'Irritability', 'class']]
new_set = new_set.rename(columns={'Genital thrush': 'Genital_thrush'})
new_set

# adjusting the x_train and x_test to be the selected features
X = new_set.iloc[:,:-1]
Y = new_set.iloc[:,-1]

#Splitting training and testing sets
X_train,X_test,Y_train,Y_test=train_test_split(X, Y,test_size=0.2, random_state=42)

"""Random Forest"""

# defining the search area
search_area={'n_estimators':[10,40,70,90,120],'max_depth':[2,3,5,10,20]}

# GridSearch
d_grid_search=GridSearchCV(estimator=RandomForestClassifier(), param_grid=search_area,cv=10, n_jobs=1,verbose=2,scoring='accuracy')
d_grid_search=d_grid_search.fit(X_train,X_train)

# the best estimator obtained by gridsearch
d_best_estimator= d_grid_search.best_estimator_
d_best_estimator

# Fit the best retrieved parameters into a random forest model
d_randomf = RandomForestClassifier(n_estimators=90 , max_depth=5,random_state=42)
d_randomf.fit(X_train,Y_train)   # fiting the model using training set

# predictions
d_y_pred = d_randomf.predict(X_test)
d_y_pred

# Calculating the accuracy of the model
d_accuracy = accuracy_score(Y_test, d_y_pred)
print("Accuracy:", d_accuracy)

# Calculating the precision score  of the model
precision = precision_score(Y_test, d_y_pred)
print("Precision:", precision)

# Calculating the recall score  of the model
recall = recall_score(Y_test, d_y_pred)
print("Recall:", recall)

#  Calculating the f1_score of the model
f1 = f1_score(Y_test, d_y_pred)
print("F1 Score:", f1)

"""Support vector machine"""

# Creating an SVM model
# d_svm_model = SVC(kernel='rbf', C=1)
d_svm_model = SVC(kernel='linear', C=1)

# Training the model on the training set
d_svm_model.fit(X_train, Y_train)

import pickle

with open('dbs_svm_model.pkl', 'wb') as f:
    pickle.dump(d_svm_model, f)

# Making predictions on the testing set
s_y_predicted = d_svm_model.predict(X_test)
s_y_predicted

# Calculating the accuracy of the model
s_accuracy = accuracy_score(Y_test, s_y_predicted)
print("Accuracy:", s_accuracy)

# Calculating the precision score  of the model
precision = precision_score(Y_test, s_y_predicted)
print("Precision:", precision)

# Calculating the recall score  of the model
recall = recall_score(Y_test, s_y_predicted)
print("Recall:", recall)

#  Calculating the f1_score of the model
f1 = f1_score(Y_test, s_y_predicted)
print("F1 Score:", f1)

"""ANN"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
cnnX_train = sc.fit_transform(X_train)
cnnX_test = sc.transform(X_test)

import pickle
import keras
from keras.models import Sequential
from keras.layers import Dense

from keras.layers import Dropout

classifier = Sequential()

#adding the first hidden layer
classifier.add(Dense(32,activation='relu',input_dim=cnnX_train.shape[1]))
classifier.add(Dropout(0.5))

#adding the second hidden layer
classifier.add(Dense(16,activation='relu'))
classifier.add(Dropout(0.5))

#adding the output layer
classifier.add(Dense(1,activation='sigmoid'))

classifier.compile(optimizer="Adam", loss='binary_crossentropy', metrics=['accuracy'])

classifier.fit(cnnX_train, Y_train, batch_size=35, epochs=60)

cnny_pred = classifier.predict(cnnX_test)
cnny_pred = (cnny_pred > 0.5)

# Making predictions on the testing set
# ypredicted = (model.predict(x_test) > 0.5).astype("int32")


# Evaluating the model on the testing set
accuracy = accuracy_score(Y_test, cnny_pred)
print("The Accuracy:", accuracy)

# Calculating performance metrics
precision = precision_score(Y_test, cnny_pred)
recall = recall_score(Y_test, cnny_pred)
f1 = f1_score(Y_test, cnny_pred)

# Printing the performance metrics
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)




# from sklearn.metrics import confusion_matrix
# cm = confusion_matrix(Y_test, cnny_pred)
# print(cm)

"""Naive bayes"""

from sklearn.naive_bayes import GaussianNB

from sklearn.naive_bayes import GaussianNB
# Creating an NB model
nb_model = GaussianNB()

# Training the model on the training set
nb_model.fit(X_train, Y_train)

# Making predictions on the testing set
ny_predicted = nb_model.predict(X_test)
ny_predicted

# Calculating the accuracy of the model
accuracy3 = accuracy_score(Y_test, ny_predicted)
print("Accuracy of nb model :", accuracy3)

# Calculating the precision score  of the model
precision = precision_score(Y_test, ny_predicted)
print("Precision:", precision)

# Calculating the recall score  of the model
recall = recall_score(Y_test, ny_predicted)
print("Recall:", recall)

#  Calculating the f1_score of the model
f1 = f1_score(Y_test, ny_predicted)
print("F1 Score:", f1)

